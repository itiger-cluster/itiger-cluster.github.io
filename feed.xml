<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://itiger-cluster.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://itiger-cluster.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-17T02:20:59+00:00</updated><id>https://itiger-cluster.github.io//feed.xml</id><title type="html">iTiger</title><subtitle>Building an HPC to Help the Mid-South. </subtitle><entry><title type="html">How to Run a LLaMA Chatbot on the Cluster</title><link href="https://itiger-cluster.github.io//tutorials/2025/llama-chatbot/" rel="alternate" type="text/html" title="How to Run a LLaMA Chatbot on the Cluster"/><published>2025-08-16T16:00:00+00:00</published><updated>2025-08-16T16:00:00+00:00</updated><id>https://itiger-cluster.github.io//tutorials/2025/llama-chatbot</id><content type="html" xml:base="https://itiger-cluster.github.io//tutorials/2025/llama-chatbot/"><![CDATA[<p>In our <a href="https://itiger-cluster.github.io/tutorials/2024/run-model/">previous tutorial</a>, we covered how to set up the environment and run a classification model using <code class="language-plaintext highlighter-rouge">sbatch</code>.<br/> This time, we will run a <strong>Meta LLaMA 3.1-8B-Instruct</strong> chatbot interactively via <code class="language-plaintext highlighter-rouge">srun</code> on the cluster.<br/> We will start with a minimal <strong>terminal chatbot</strong>, then upgrade it to a <strong>web interface</strong>.</p> <h2 id="environment-setup">Environment Setup</h2> <ol> <li>Create a conda environment Make sure you are working under the <code class="language-plaintext highlighter-rouge">/project</code> directory (to avoid <code class="language-plaintext highlighter-rouge">/home</code> quota issues).<br/> Then create a dedicated conda environment and install the required libraries:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /project/your_username/
<span class="nb">mkdir</span> <span class="nt">-p</span> llama-chatbot
<span class="nb">cd </span>llama-chatbot
conda create <span class="nt">-n</span> llama_chat <span class="nv">python</span><span class="o">=</span>3.10 <span class="nt">-y</span>
conda activate llama_chat
</code></pre></div></div> <ol> <li>Install Hugging Face ecosystem and Gradio pip install -U ‚Äútransformers&gt;=4.44.0‚Äù ‚Äúaccelerate&gt;=0.33.0‚Äù ‚Äútokenizers&gt;=0.19.0‚Äù safetensors gradio</li> </ol> <h2 id="hugging-face-access">Hugging Face Access</h2> <p>Meta‚Äôs LLaMA models require a license agreement and authentication.</p> <h4 id="accept-the-license">Accept the license</h4> <p>Visit: <a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">meta-llama/Llama-3.1-8B-Instruct</a> Fill the form and click Submit</p> <h4 id="create-an-access-token">Create an access token</h4> <ol> <li>Log in with your Huggingface account.</li> <li>Go to Hugging Face <a href="https://huggingface.co/settings/tokens">Access Tokens Page</a>.</li> <li>Create a read token (hf_‚Ä¶), make sure the Llama-3.1-8B-Instruct model is selected and copy the token.</li> <li>Log in on the cluster <code class="language-plaintext highlighter-rouge">huggingface-cli login</code> Paste your token when prompted. You should see: ‚ÄúLogin successful. The current active token is: <code class="language-plaintext highlighter-rouge">XXX</code>‚Äù.</li> </ol> <h2 id="run-a-chatbot-in-your-terminal">Run a Chatbot in your terminal</h2> <ol> <li>Prepare the script</li> </ol> <p>Save the following script in your prject directory, e.g. <code class="language-plaintext highlighter-rouge">/project/your_username/llama-chatbot/llama_chat.py</code>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from transformers import AutoTokenizer, AutoModelForCausalLM
import torch, sys

MODEL_ID = "meta-llama/Llama-3.1-8B-Instruct"

tok = AutoTokenizer.from_pretrained(MODEL_ID, use_auth_token=True)
mdl = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    attn_implementation="sdpa",
    use_auth_token=True,
)

print("üí¨ LLaMA Lab Assistant ‚Äî type 'exit' to quit")
history = []
while True:
    user = input("You: ").strip()
    if user.lower() == "exit":
        sys.exit(0)

    messages = history + [{"role": "user", "content": user}]
    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tok(prompt, return_tensors="pt").to(mdl.device)

    with torch.no_grad():
        out = mdl.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            eos_token_id=tok.eos_token_id,
        )
    text = tok.decode(out[0], skip_special_tokens=True)
    reply = text.split(prompt, 1)[-1].strip()
    print(reply)
    history.append({"role": "user", "content": user})
    history.append({"role": "assistant", "content": reply})
</code></pre></div></div> <ol> <li>Run it interactively on a GPU node:</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun --gres=gpu:1 --cpus-per-task=4 --mem=64G --time=2:00:00 --partition=bigTiger \
     python /project/your_username/tutorial/llama-chatbot/llama_chat.py
</code></pre></div></div> <ol> <li>Start Conversations:</li> </ol> <p>Once the chatbot starts, you can interact in the terminal, for example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>üí¨ LLaMA Lab Assistant ‚Äî type 'exit' to quit

You: Can you explain what a GPU is?
Assistant: A Graphics Processing Unit (GPU) is a specialized electronic circuit designed to quickly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device...

exit
</code></pre></div></div> <h2 id="upgrade-to-a-web-interface-gradio">Upgrade to a Web Interface (Gradio)</h2> <p>For a more user-friendly interface, we can use Gradio to run the chatbot in a browser.</p> <ol> <li>Prepare the script</li> </ol> <p>Save this script as /project/your_username/tutorial/llama-chatbot/llama_gradio.py, and <strong>change line 6</strong> to your actual directory:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import os, socket, gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Put HF cache under /project to avoid home quota and deprecation warning
os.environ.setdefault("HF_HOME", "/your_username/wliu9/.cache/huggingface")

MODEL_ID = "meta-llama/Llama-3.1-8B-Instruct"
PORT = 7861  # change if the port is busy

# You already did `huggingface-cli login`, no explicit token needed here
tok = AutoTokenizer.from_pretrained(MODEL_ID)
mdl = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    attn_implementation="sdpa",
)

def chat_fn(message, history):
    """
    Gradio passes:
      - message: current user input (str)
      - history: list of (user, assistant) tuples
    Convert to LLaMA chat template messages.
    """
    msgs = []
    for u, b in (history or []):
        if u:
            msgs.append({"role": "user", "content": u})
        if b:
            msgs.append({"role": "assistant", "content": b})
    msgs.append({"role": "user", "content": message})

    prompt = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
    inputs = tok(prompt, return_tensors="pt").to(mdl.device)

    with torch.no_grad():
        out = mdl.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            eos_token_id=tok.eos_token_id,
        )

    text = tok.decode(out[0], skip_special_tokens=True)
    reply = text.split("assistant\n", 1)[-1].strip()
    return reply

print(f"[Gradio] compute node = {socket.gethostname()}, port = {PORT}", flush=True)

demo = gr.ChatInterface(fn=chat_fn, title="LLaMA Lab Assistant")
demo.launch(server_name="0.0.0.0", server_port=PORT, show_error=True)

</code></pre></div></div> <ol> <li>Run it on a GPU node</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun --gres=gpu:1 --cpus-per-task=4 --mem=64G --time=2:00:00 --partition=bigTiger \
     python /project/your_username/tutorial/llama-chatbot/llama_gradio.py
</code></pre></div></div> <p>This will start the LLaMA chatbot on a GPU node. In the terminal you should see output like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Gradio] compute node = itiger01, port = 7861
Running on local URL:  http://0.0.0.0:7861
</code></pre></div></div> <ol> <li>Connect from your local machine</li> </ol> <p>Open a new terminal on your local computer and create an SSH tunnel. Replace <code class="language-plaintext highlighter-rouge">itiger01</code> with the actual compute node name printed above, and and <code class="language-plaintext highlighter-rouge">you_username</code> as your actual username:</p> <p><code class="language-plaintext highlighter-rouge">ssh -Nf -L 17861:itiger01:7861 you_username@itiger.memphis.edu</code></p> <p>Now open a browser on your local machine and go to: <code class="language-plaintext highlighter-rouge">http://localhost:17861</code></p> <p>You should see the LLaMA Lab Assistant Gradio interface, where you can start chatting with the model interactively.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/llama_chatbot-480.webp 480w,/assets/img/llama_chatbot-800.webp 800w,/assets/img/llama_chatbot-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/llama_chatbot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[run a chatbot, srun]]></summary></entry><entry><title type="html">How to Run a Text Classification Model</title><link href="https://itiger-cluster.github.io//tutorials/2024/run-model/" rel="alternate" type="text/html" title="How to Run a Text Classification Model"/><published>2024-10-08T15:00:00+00:00</published><updated>2024-10-08T15:00:00+00:00</updated><id>https://itiger-cluster.github.io//tutorials/2024/run-model</id><content type="html" xml:base="https://itiger-cluster.github.io//tutorials/2024/run-model/"><![CDATA[<h2 id="environment-setup">Environment Setup</h2> <h3 id="install-conda-in-the-project-directory">Install Conda in the <code class="language-plaintext highlighter-rouge">/project</code> Directory</h3> <ol> <li>We recommend using <strong>Miniconda</strong> for a lightweight and efficient Python environment setup. To install Miniconda3 in the <code class="language-plaintext highlighter-rouge">/project</code> directory (recommended to avoid the 20 GB storage limitations in <code class="language-plaintext highlighter-rouge">/home</code>), run the following commands:</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm ~/miniconda3/miniconda.sh
</code></pre></div></div> <p>These four commands download the 64-bit version of the Linux installer, rename it to a shorter file name, silently install it, and then delete the installer.</p> <p>You may download a different version of Miniconda by following the instructions available at <a href="https://docs.anaconda.com/miniconda/">Miniconda Documentation</a>.</p> <p>And add to your system‚Äôs $PATH environment variable by running:</p> <p><code class="language-plaintext highlighter-rouge">export PATH=~/miniconda3/bin:$PATH</code></p> <p>To ensure that Miniconda is always available, add the above line to your shell configuration file (.bashrc) by running:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo 'export PATH=~/miniconda3/bin:$PATH' &gt;&gt; ~/.bashrc
conda init
source ~/.bashrc
</code></pre></div></div> <h3 id="creating-a-new-conda-environment">Creating a new Conda Environment</h3> <p>Create a new Conda environment with Python 3.10 by running the following command:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create -n myenv python=3.10
conda activate myenv
</code></pre></div></div> <p>After activating the environment, install pytorch with CUDA 11.8 support and other necessary dependencies:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda install pytorch torchvision torchaudio pytorch-cuda=11.8 scikit-learn -c pytorch -c nvidia
pip install transformers==4.35.0 evaluate==0.4.1 datasets
</code></pre></div></div> <h3 id="running-a-task">Running a Task:</h3> <p>We show an example of how to run a text classification task using pretrained language model and public dataset on <a href="https://huggingface.co/">Huggingface</a>:</p> <p>Download the classification python code from <a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_classification.py">here</a>, change branch to v4.35-release before downloading and put the scripts under your directory.</p> <h2 id="preparing-a-slurm-submission-script">Preparing a SLURM submission script</h2> <p>Create a shell script file (e.g., by running <code class="language-plaintext highlighter-rouge">touch run_job.sh</code> in the terminal) and open it for editing. Below is an example script that you can copy:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash
#SBATCH --job-name=text_classification       # Job name
#SBATCH --output=output_%j.txt               # Output log file (%j will be replaced by the job ID)
#SBATCH --error=error_%j.txt                 # Error log file
#SBATCH --ntasks=1                           # Number of tasks (typically 1 for a single Python script)
#SBATCH --cpus-per-task=4                    # Number of CPU cores per task
#SBATCH --gres=gpu:1                         # Request 1 GPU (adjust based on the available resources)
#SBATCH --mem=16G                            # Memory allocation
#SBATCH --time=12:00:00                      # Maximum run time (HH:MM:SS)
#SBATCH --partition=bigTiger                 # Partition to submit to (adjust based on availability)
# Set dataset and subset variables
dataset="imdb"
# Run the classification task using the dataset and subset variables
python run_classification.py \
    --model_name_or_path google-bert/bert-base-uncased \
    --dataset_name ${dataset} \
    --trust_remote_code True \
    --shuffle_train_dataset \
    --metric_name accuracy \
    --text_column_name text \
    --do_train \
    --do_eval \
    --max_seq_length 512 \
    --per_device_train_batch_size 32 \
    --learning_rate 2e-5 \
    --num_train_epochs 1 \
    --output_dir /tmp/${dataset}/
</code></pre></div></div> <h2 id="submit-the-task">Submit the Task</h2> <p>Use the following command to submit the job:</p> <p><code class="language-plaintext highlighter-rouge">sbatch run_job.sh</code></p> <p>After submitting the job, you can view detailed information about a specific job (replace <job_id> with your actual job ID) with the following commands:</job_id></p> <p><code class="language-plaintext highlighter-rouge">scontrol show job &lt;job_id&gt;</code></p>]]></content><author><name></name></author><summary type="html"><![CDATA[environment setup, submitting a job]]></summary></entry><entry><title type="html">How to View Your Virtual Desktop + Tips</title><link href="https://itiger-cluster.github.io//tutorials/2024/gui/" rel="alternate" type="text/html" title="How to View Your Virtual Desktop + Tips"/><published>2024-09-19T15:00:00+00:00</published><updated>2024-09-19T15:00:00+00:00</updated><id>https://itiger-cluster.github.io//tutorials/2024/gui</id><content type="html" xml:base="https://itiger-cluster.github.io//tutorials/2024/gui/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/folders-480.webp 480w,/assets/img/folders-800.webp 800w,/assets/img/folders-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/folders.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To be able to view your virtual desktop on the HPC, install <a href="&quot;https://wiki.x2go.org/doku.php&quot;">X2go</a> and choose the version for your computer‚Äôs operating system.</p> <p>Once you open X2go, you should see the below. For ‚Äúhost‚Äù, enter in ‚Äúitiger.memphis.edu‚Äù. Change the bottom left option from <code class="language-plaintext highlighter-rouge">KDE</code> to <code class="language-plaintext highlighter-rouge">XFCE</code>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/session_memphis-480.webp 480w,/assets/img/session_memphis-800.webp 800w,/assets/img/session_memphis-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/session_memphis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Save this, then click on the button named ‚ÄúNew session‚Äù on the right of the below tab:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/session_memphis_2-480.webp 480w,/assets/img/session_memphis_2-800.webp 800w,/assets/img/session_memphis_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/session_memphis_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Enter in your credentials, and you should be set!</p> <h3>Virtual Desktop Tips</h3> <p>You have the ability to run programs and submit jobs within the virtual desktop (although this may be slower than submitting jobs from your own ssh terminal :)</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Prefer a GUI as opposed to the terminal? Read this!]]></summary></entry><entry><title type="html">Jupyter Notebook on the HPC</title><link href="https://itiger-cluster.github.io//tutorials/2024/jupyter-notebook/" rel="alternate" type="text/html" title="Jupyter Notebook on the HPC"/><published>2024-09-17T00:00:00+00:00</published><updated>2024-09-17T00:00:00+00:00</updated><id>https://itiger-cluster.github.io//tutorials/2024/jupyter-notebook</id><content type="html" xml:base="https://itiger-cluster.github.io//tutorials/2024/jupyter-notebook/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Jupyter Notebooks are often used in computational research and data science projects. Here‚Äôs how to set it up on the HPC:</p> <hr/> <h4>(Recommended) Creating a Python Virtual Environment</h4> <p>Creating a virtual environment before running a jupyter-notebook instance is recommended for module cleanliness‚Äìin addition, it is very simple to create one!</p> <p>In the terminal, type `python -m venv [your desired directory]. The picture below shows an example directory you could try:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/create_venv-480.webp 480w,/assets/img/create_venv-800.webp 800w,/assets/img/create_venv-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/create_venv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, <code class="language-plaintext highlighter-rouge">cd</code> into your folder. You should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inside_venv_folder-480.webp 480w,/assets/img/inside_venv_folder-800.webp 800w,/assets/img/inside_venv_folder-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/inside_venv_folder.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, we must install the required dependencies(in this case, jupyter). In the terminal, simply run <code class="language-plaintext highlighter-rouge">pip3 install jupyter</code> to install the required modules.</p> <p>Finally, we‚Äôll kickstart this jupyter instance through running a <code class="language-plaintext highlighter-rouge">.sh</code> file with <code class="language-plaintext highlighter-rouge">sbatch</code>. Create a new file to submit to the cluster, and make sure to include <code class="language-plaintext highlighter-rouge">~/[directory]/bin/activate</code>, as well as <code class="language-plaintext highlighter-rouge">module load python/[your_version]</code>.</p> <p>Now, type in <code class="language-plaintext highlighter-rouge">jupyter notebook</code> into the file. save it, then submit it with <code class="language-plaintext highlighter-rouge">sbatch</code>. For context, this is an example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash
#SBATCH --cpus-per-task=1
#SBATCH --mem=10G
#SBATCH --time=1-00:00:00
#SBATCH --gres=gpu:1
#SBATCH --partition=agpuq

#module load nvhpc/23.11
#module load cuda/12.3
module load python/3.12.1

. ~/pythonGPU/bin/activate

echo "*** Starting Jupyter on: "$(hostname)
jupyter notebook
</code></pre></div></div> <p>Once running, you should see a new .out file. Open it up, and you should see something like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/jupyter_notebook_runs-480.webp 480w,/assets/img/jupyter_notebook_runs-800.webp 800w,/assets/img/jupyter_notebook_runs-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/jupyter_notebook_runs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="accessing-your-jupyter-notebook">Accessing your Jupyter Notebook</h2> <p>Now that we have the instance running, we need to be able to view it on a browser tab, since if you try to use the link on a normal browser, it will not work, since we need specialized access.</p> <p>The most convenient way to view your instances is through connecting to the cluster with X2go. To install X2go, simply download the version applicable to your device from <a href="&quot;https://wiki.x2go.org/doku.php&quot;">this link</a>. Once you do that, open up the application, and enter in the required parameters similar to the picture below. This is for the new session we will create. Make SURE to change the bottom parameter from the initial <code class="language-plaintext highlighter-rouge">KDE</code> to the <code class="language-plaintext highlighter-rouge">XFCE</code> option.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/x2go-480.webp 480w,/assets/img/x2go-800.webp 800w,/assets/img/x2go-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/x2go.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Pro tip: Notice that the host name is <strong>bigblue1.memphis.edu</strong>, and not <strong>bigblue.memphis.edu</strong>? This is because we actually have two login nodes‚Äìand they are represented by the corresponding numbers <em>1</em> and <em>2</em>. If you wish to run both the virtual desktop session AND a terminal session at the same time, check for the last two digits after the @. If they say 01, make the host <strong>bigblue2.memphis.edu</strong>, and vice versa. This is <strong>optional</strong>, not required.</p> </blockquote> <p>Now, continue by pressing OK.</p> <p>You should see something like this, where the box on the right represents the session you want to start. Click on it.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/client2-480.webp 480w,/assets/img/client2-800.webp 800w,/assets/img/client2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/client2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, enter in your credentials, and you should see a window open, looking like a virtual desktop.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/folders-480.webp 480w,/assets/img/folders-800.webp 800w,/assets/img/folders-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/folders.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>On the bottom bar, click on the browser option. There, a web browser should open. From there, paste the jupyter link the <code class="language-plaintext highlighter-rouge">.out</code> file gave when we submitted that batch script. From there, you should be able to see your jupyter notebook running!</p> <blockquote> <p>Don‚Äôt forget to close your jupyter notebook instance when you are done. You can do this by running <code class="language-plaintext highlighter-rouge">scancel [your job id]</code> in the terminal.</p> </blockquote> <hr/>]]></content><author><name>Mayira Sharif</name></author><category term="user-tutorials"/><summary type="html"><![CDATA[How to run a jupyter notebook instance on the HPC]]></summary></entry><entry><title type="html">Submitting a Python Job with Hello World</title><link href="https://itiger-cluster.github.io//tutorials/2024/hello-world/" rel="alternate" type="text/html" title="Submitting a Python Job with Hello World"/><published>2024-04-04T15:00:00+00:00</published><updated>2024-04-04T15:00:00+00:00</updated><id>https://itiger-cluster.github.io//tutorials/2024/hello-world</id><content type="html" xml:base="https://itiger-cluster.github.io//tutorials/2024/hello-world/"><![CDATA[<p><strong>Let‚Äôs learn how to submit a job to the HPC!</strong></p> <p><br/> To start, open up an IDE of your choice and write code that will output ‚ÄúHello World‚Äù into the terminal:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Hello World")
</code></pre></div></div> <p>Next, upload your file onto the HPC. Depending on your system/OS, you may have to follow different steps to put your file onto the HPC. For that, follow this <a href="https://itiger-cluster.github.io//blog/2024/images/">tutorial.</a></p> <p><br/> <br/></p> <p>Next, we must create a basic BASH file to submit to the HPC. The BASH file acts as sort of a set of instructions for the HPC, including what resources the HPC will provide, the length of the job we shall submit, the modules we will need for the job (in this case, python 3.8.7) and the file we will run (our python file, which in this case is animal.py).</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=1G
#SBATCH --partition=acomputeq
#SBATCH --job-name=basicname

echo "*** Running Python on: "$(hostname)

python animal.py
</code></pre></div></div> <p>After we include the right instructions into our BASH file, we are now able to upload/add it to our home directory of the HPC (<a href="https://itiger-cluster.github.io/blog/2024/images/">link for how to transfer files to the HPC</a> and use the command <code class="language-plaintext highlighter-rouge">sbatch</code> to begin the job. Once the job has submitted and finished, you should recieve an output file within your directory‚Äìthis contains the ‚Äúhello world‚Äù output you‚Äôd typically see on your terminal:</p> <p align="center"> <img src="https://itiger-cluster.github.io/assets/img/helloworld2-1400.webp"/> </p> <p>Open the file in a text editor, and you should see your output!</p> <p align="center"> <img src="https://itiger-cluster.github.io/assets/img/helloworld3-1400.webp"/> </p>]]></content><author><name></name></author><summary type="html"><![CDATA[Learn how to submit a job with Python]]></summary></entry><entry><title type="html">Creating an Account</title><link href="https://itiger-cluster.github.io//tutorials/2024/account/" rel="alternate" type="text/html" title="Creating an Account"/><published>2024-03-26T00:00:00+00:00</published><updated>2024-03-26T00:00:00+00:00</updated><id>https://itiger-cluster.github.io//tutorials/2024/account</id><content type="html" xml:base="https://itiger-cluster.github.io//tutorials/2024/account/"><![CDATA[<h2 id="completing-the-form">Completing the Form</h2> <p>Hello, and thank you for your interest in iTiger! To begin the process of gaining access to the iTiger HPC, you must begin with filling out the form found on the <a href="https://itiger-cluster.github.io/requests/">requests page</a>. Note that the form is different for internal and external users (in this case, ‚Äúinternal‚Äù means you are affiliated with the University of Memphis and ‚Äúexternal‚Äù means you are not affiliated).</p> <p>Please wait for 3 - 5 business days to recieve a response from the University of Memphis IT department.</p> <hr/> <h2 id="setting-up-software--terminal">Setting Up Software / Terminal</h2> <p>Once you have recieved an email from IT, your next steps are to set up your device to connect to the HPC.</p> <p><strong>For External Users:</strong> If you are an external user, it is important to know that the only way to access the UofM HPC is by being connected to the University of Memphis network by VPN. Setting up this VPN is relatively simple‚Äìfollow these <a href="https://www.memphis.edu/umtech/solutions/vpn.php">tutorials</a> provided by the University of Memphis.</p> <p><strong>For Windows Users:</strong> In order to access the HPC, you must download either <a href="http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html">PuTTY</a> or <a href="https://mobaxterm.mobatek.net/">mobaXterm</a>.</p> <p>Once you have installed PuTTY, execute the program, and you should see a window that looks like this: <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/configuration-480.webp 480w,/assets/img/configuration-800.webp 800w,/assets/img/configuration-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/configuration.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For the entry titled <code class="language-plaintext highlighter-rouge">Host Name</code>, enter <code class="language-plaintext highlighter-rouge">itiger.memphis.edu</code>. Keep the other options as default, and click on <code class="language-plaintext highlighter-rouge">Open</code>.</p> <p><br/></p> <p>From then, you‚Äôll see a new terminal window open up. When it says <code class="language-plaintext highlighter-rouge">login as:</code>, put in your UofM username. The password it asks for next is just your password that you use to login to your UofM account.</p> <p><br/> <strong>For Linux and MacOS:</strong> You can gain access by logging in with ‚Äússh‚Äù via the terminal or shell:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh [username]@hpclogin.memphis.edu
</code></pre></div></div> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh [username]@itiger.memphis.edu
</code></pre></div></div> <p>In addition, X2Go works on both MacOS and Windows. Here is how to connect to the HPC:</p> <ul> <li>Host: hpclogin.memphis.edu</li> <li>Login: username[your UofM username]</li> <li>SSH Port: 22</li> <li>Session Type: XFCE</li> </ul> <p>In both cases, after you entered in your valid login information, you should be logged into the terminal!</p> <p>You have now learned how to connect to the HPC!</p>]]></content><author><name>Weisi Liu</name></author><summary type="html"><![CDATA[How to set up your account on the HPC]]></summary></entry></feed>