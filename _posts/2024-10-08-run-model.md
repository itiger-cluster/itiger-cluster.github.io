---
layout: post
title: How to Run a Text Classification Model
date: 2024-10-08 15:00:00
description: environment setup, submitting a job
tags: getting-started images
categories: tutorial
---
## Environment Setup

### Download & Install Conda:

In your directory, install Miniconda3 by copying the following command:

```
mkdir -p ~/miniconda3 #creating a new directory 
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh #installing the Linux x64 v.
-O ~/miniconda3/miniconda.sh #output into the specified folder
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm ~/miniconda3/miniconda.s # rename into a shorter file name
```

And add to your system's $PATH environment variable by running:

`export PATH=~/miniconda3/bin:$PATH`

To ensure that Miniconda is always available, add the above line to your shell 
configuration file (.bashrc)by running:

```
echo 'export PATH=~/miniconda3/bin:$PATH' >> ~/.bashrc
conda init
source ~/.bashrc
```

### Creating a new Conda Environment

```
conda create -n myenv python=3.10
conda activate myenv
```

Install pytorch and other libraries:
```
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 scikit-
learn -c pytorch -c nvidia
pip install transformers==4.35.0 evaluate==0.4.1 datasets
```

### Running a Task:

Below are instructions to run a text classification class using a public dataset on <a href="https://huggingface.co/">Huggingface</a>: 
We download the classification python code <a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_classification.py">here</a>:
## Preparing a SLURM submission script
```
#!/bin/bash
#SBATCH --job-name=text_classification       # Job name
#SBATCH --output=output_%j.txt               # Output log file (%j will be replaced by the job ID)
#SBATCH --error=error_%j.txt                 # Error log file
#SBATCH --ntasks=1                           # Number of tasks (typically 1 for a single Python script)
#SBATCH --cpus-per-task=4                    # Number of CPU cores per task
#SBATCH --gres=gpu:1                         # Request 1 GPU (adjust based on the available resources)
#SBATCH --mem=16G                            # Memory allocation
#SBATCH --time=12:00:00                      # Maximum run time (HH:MM:SS)
#SBATCH --partition=bigTiger                 # Partition to submit to (adjust based on availability)
# Set dataset and subset variables
dataset="imdb"
# Run the classification task using the dataset and subset variables
python run_classification.py \
    --model_name_or_path google-bert/bert-base-uncased \
    --dataset_name ${dataset} \
    --trust_remote_code True \
    --shuffle_train_dataset \
    --metric_name accuracy \
    --text_column_name text \
    --do_train \
    --do_eval \
    --max_seq_length 512 \
    --per_device_train_batch_size 32 \
    --learning_rate 2e-5 \
    --num_train_epochs 1 \
    --output_dir /tmp/${dataset}/
```

## Submit the Task
`sbatch run_job.sh`


